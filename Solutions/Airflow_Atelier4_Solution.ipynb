{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Construction d'une pipeline d'exécution avec PostgreSQL "
      ],
      "metadata": {
        "id": "jFmbMwx4YVQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![atelier4](https://user-images.githubusercontent.com/123757632/231912146-232774b4-c30c-4901-b4c1-ea27db59122b.png)"
      ],
      "metadata": {
        "id": "hE56uhOTYWiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tâches de création de table\n",
        "\n",
        "Il est possible d'utiliser PostgresOperator afin de définir des tâches qui créeront des tables dans la base de données postgres.\n",
        "\n",
        "Il est prévu de créer deux tables : une pour faciliter le nettoyage des données (employees_temp) et une autre pour stocker les données nettoyées (employees)."
      ],
      "metadata": {
        "id": "UjgMOz6oZG_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwIBDoKvYP59"
      },
      "outputs": [],
      "source": [
        "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
        "\n",
        "create_employees_table = PostgresOperator(\n",
        "    task_id=\"create_employees_table\",\n",
        "    postgres_conn_id=\"tutorial_pg_conn\",\n",
        "    sql=\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS employees (\n",
        "            \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "            \"Company Name\" TEXT,\n",
        "            \"Employee Markme\" TEXT,\n",
        "            \"Description\" TEXT,\n",
        "            \"Leave\" INTEGER\n",
        "        );\"\"\",\n",
        ")\n",
        "\n",
        "create_employees_temp_table = PostgresOperator(\n",
        "    task_id=\"create_employees_temp_table\",\n",
        "    postgres_conn_id=\"tutorial_pg_conn\",\n",
        "    sql=\"\"\"\n",
        "        DROP TABLE IF EXISTS employees_temp;\n",
        "        CREATE TABLE employees_temp (\n",
        "            \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "            \"Company Name\" TEXT,\n",
        "            \"Employee Markme\" TEXT,\n",
        "            \"Description\" TEXT,\n",
        "            \"Leave\" INTEGER\n",
        "        );\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tâche de récupération de données\n",
        "\n",
        "Ici, les données sont récupérées, enregistrées dans un fichier sur l'instance Airflow, puis chargées à partir de ce fichier dans une table intermédiaire afin de pouvoir exécuter les étapes de nettoyage des données."
      ],
      "metadata": {
        "id": "upY50tPvZLGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from airflow.decorators import task\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "\n",
        "\n",
        "@task\n",
        "def get_data():\n",
        "    # REMARQUE : configurez cela selon les besoins de votre environnement Airflow.\n",
        "    data_path = \"/opt/airflow/dags/files/employees.csv\"\n",
        "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\"\n",
        "\n",
        "    response = requests.request(\"GET\", url)\n",
        "\n",
        "    with open(data_path, \"w\") as file:\n",
        "        file.write(response.text)\n",
        "\n",
        "    postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
        "    conn = postgres_hook.get_conn()\n",
        "    cur = conn.cursor()\n",
        "    with open(data_path, \"r\") as file:\n",
        "        cur.copy_expert(\n",
        "            \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n",
        "            file,\n",
        "        )\n",
        "    conn.commit()"
      ],
      "metadata": {
        "id": "82NvpgKuZNRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tâche de fusion de données\n",
        "Ici, des enregistrements complètement uniques sont sélectionnés à partir des données récupérées, puis il est vérifié si des numéros de série d'employés sont déjà présents dans la base de données. Si tel est le cas, ces enregistrements sont mis à jour avec les nouvelles données."
      ],
      "metadata": {
        "id": "wb_j7GcyZRik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow.decorators import task\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "\n",
        "\n",
        "@task\n",
        "def merge_data():\n",
        "    query = \"\"\"\n",
        "        INSERT INTO employees\n",
        "        SELECT *\n",
        "        FROM (\n",
        "            SELECT DISTINCT *\n",
        "            FROM employees_temp\n",
        "        ) t\n",
        "        ON CONFLICT (\"Serial Number\") DO UPDATE\n",
        "        SET\n",
        "              \"Employee Markme\" = excluded.\"Employee Markme\",\n",
        "              \"Description\" = excluded.\"Description\",\n",
        "              \"Leave\" = excluded.\"Leave\";\n",
        "    \"\"\"\n",
        "    try:\n",
        "        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
        "        conn = postgres_hook.get_conn()\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(query)\n",
        "        conn.commit()\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "4yhYtQaqZSc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalisation du DAG\n",
        "\n",
        "Les tâches ont été développées et doivent maintenant être intégrées dans un DAG. Ce DAG permet de définir quand et comment les tâches doivent s'exécuter, ainsi que les dépendances entre les tâches. Le DAG ci-dessous est configuré pour :\n",
        "\n",
        "* s'exécuter tous les jours à minuit à partir du 24er mars 2023,\n",
        "\n",
        "* ne s'exécuter qu'une seule fois en cas de jours manqués, et\n",
        "\n",
        "* s'interrompre après 60 minutes.\n",
        "\n",
        "À partir de la dernière ligne de la définition du DAG \"process-employees\", il est possible de voir :"
      ],
      "metadata": {
        "id": "cgocEngSZU3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[create_employees_table, create_employees_temp_table] >> get_data() >> merge_data()"
      ],
      "metadata": {
        "id": "qh_l_fqaZXRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* la tâche \"merge_data()\" dépend de la tâche \"get_data()\",\n",
        "\n",
        "* \"get_data()\" dépend à la fois des tâches \"create_employees_table\" et \"create_employees_temp_table\",\n",
        "\n",
        "* et les tâches \"create_employees_table\" et \"create_employees_temp_table\" peuvent s'exécuter indépendamment.\n",
        "\n",
        "En combinant tous ces éléments, le DAG est finalisé."
      ],
      "metadata": {
        "id": "bksL7gSzZcrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import pendulum\n",
        "import os\n",
        "\n",
        "import requests\n",
        "from airflow.decorators import dag, task\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
        "\n",
        "\n",
        "@dag(\n",
        "    dag_id=\"process-employees\",\n",
        "    schedule_interval=\"0 0 * * *\",\n",
        "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
        "    catchup=False,\n",
        "    dagrun_timeout=datetime.timedelta(minutes=60),\n",
        ")\n",
        "def ProcessEmployees():\n",
        "    create_employees_table = PostgresOperator(\n",
        "        task_id=\"create_employees_table\",\n",
        "        postgres_conn_id=\"tutorial_pg_conn\",\n",
        "        sql=\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS employees (\n",
        "                \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "                \"Company Name\" TEXT,\n",
        "                \"Employee Markme\" TEXT,\n",
        "                \"Description\" TEXT,\n",
        "                \"Leave\" INTEGER\n",
        "            );\"\"\",\n",
        "    )\n",
        "\n",
        "    create_employees_temp_table = PostgresOperator(\n",
        "        task_id=\"create_employees_temp_table\",\n",
        "        postgres_conn_id=\"tutorial_pg_conn\",\n",
        "        sql=\"\"\"\n",
        "            DROP TABLE IF EXISTS employees_temp;\n",
        "            CREATE TABLE employees_temp (\n",
        "                \"Serial Number\" NUMERIC PRIMARY KEY,\n",
        "                \"Company Name\" TEXT,\n",
        "                \"Employee Markme\" TEXT,\n",
        "                \"Description\" TEXT,\n",
        "                \"Leave\" INTEGER\n",
        "            );\"\"\",\n",
        "    )\n",
        "\n",
        "    @task\n",
        "    def get_data():\n",
        "        # REMARQUE : configurez cela selon les besoins de votre environnement Airflow.\n",
        "        data_path = \"/opt/airflow/dags/files/employees.csv\"\n",
        "        os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "\n",
        "        url = \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\"\n",
        "\n",
        "        response = requests.request(\"GET\", url)\n",
        "\n",
        "        with open(data_path, \"w\") as file:\n",
        "            file.write(response.text)\n",
        "\n",
        "        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
        "        conn = postgres_hook.get_conn()\n",
        "        cur = conn.cursor()\n",
        "        with open(data_path, \"r\") as file:\n",
        "            cur.copy_expert(\n",
        "                \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n",
        "                file,\n",
        "            )\n",
        "        conn.commit()\n",
        "\n",
        "    @task\n",
        "    def merge_data():\n",
        "        query = \"\"\"\n",
        "            INSERT INTO employees\n",
        "            SELECT *\n",
        "            FROM (\n",
        "                SELECT DISTINCT *\n",
        "                FROM employees_temp\n",
        "            ) t\n",
        "            ON CONFLICT (\"Serial Number\") DO UPDATE\n",
        "            SET \"Serial Number\" = excluded.\"Serial Number\";\n",
        "        \"\"\"\n",
        "        try:\n",
        "            postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
        "            conn = postgres_hook.get_conn()\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(query)\n",
        "            conn.commit()\n",
        "            return 0\n",
        "        except Exception as e:\n",
        "            return 1\n",
        "\n",
        "    [create_employees_table, create_employees_temp_table] >> get_data() >> merge_data()\n",
        "\n",
        "\n",
        "dag = ProcessEmployees()"
      ],
      "metadata": {
        "id": "0Jkz2HXwZgXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enregistrez ce code dans un fichier python dans le dossier /dags (par exemple dags/process-employees.py) et (après une courte attente), le DAG process-employees sera inclus dans la liste des DAG disponibles sur l'interface utilisateur web.\n",
        "\n"
      ],
      "metadata": {
        "id": "O1mHRHrwZjwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://user-images.githubusercontent.com/123757632/227974881-9e246848-cd68-4215-9dc6-e3083042455e.png)\n"
      ],
      "metadata": {
        "id": "NDTsoZQKZoGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est possible de déclencher le DAG \"process-employees\" en le sélectionnant (via le curseur à gauche) et en le lançant (en appuyant sur le bouton Exécuter sous Actions)."
      ],
      "metadata": {
        "id": "9yT5uGssZsGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://user-images.githubusercontent.com/123757632/227974915-f7f4f7f6-6401-4ed1-a435-5c6db720d48c.png)\n"
      ],
      "metadata": {
        "id": "KSgH21jmZtc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En examinant la vue en grille du DAG \"process-employees\", il est possible de constater que toutes les tâches ont été exécutées avec succès lors de toutes les exécutions effectuées. C'est un succès !"
      ],
      "metadata": {
        "id": "zDDutpQjZwUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![image](https://user-images.githubusercontent.com/123757632/227974958-41e5b7bb-c5a0-4d22-acd3-8ae67b561586.png)"
      ],
      "metadata": {
        "id": "RqTpu4m3Zz6U"
      }
    }
  ]
}