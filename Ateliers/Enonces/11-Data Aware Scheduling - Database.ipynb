{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/kplr-training/Airflow/blob/main/Ateliers/Enonc%C3%A9s/11-Data%20Aware%20Scheduling%20-%20Database.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Dynamic DAG Creation\u202f: Cr\u00e9ation dynamique de DAGs pour \u00e9crire et lire des donn\u00e9es dans PostgreSQL en utilisant les hooks "
            ],
            "metadata": {
                "id": "4f8wqV4dAI1r"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "![pgaware_multi_dynamic_hook](https://user-images.githubusercontent.com/123757632/231905847-f4620b8c-b165-4d35-9b9e-afa22bc73a71.png)"
            ],
            "metadata": {
                "id": "jVCorx6nAZBq"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- Cr\u00e9ation dynamique : \n",
                "  -  Cr\u00e9ation de DAGs dynamiques en fonction des connexions disponibles dans Airflow pour une base de donn\u00e9es PostgreSQL. \n",
                "  - Pour chaque connexion, un DAG est cr\u00e9\u00e9 avec un nom unique \"pg_dynamic_\" suivi du nom de la connexion. \n",
                "  - Le DAG ainsi cr\u00e9\u00e9 ex\u00e9cute une t\u00e2che \"write_to_postgres\" qui se connecte \u00e0 la base de donn\u00e9es correspondante et ins\u00e8re une ligne de donn\u00e9es dans une table sp\u00e9cifique."
            ],
            "metadata": {
                "id": "8QWoIhKlu-I8"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- D\u00e9finir une fonction create_dag() qui prend en entr\u00e9e un identifiant de DAG, un horaire, une connexion PostgreSQL et des arguments par d\u00e9faut, et qui retourne un objet DAG. Cette fonction cr\u00e9e une table dans la base de donn\u00e9es PostgreSQL sp\u00e9cifi\u00e9e, ins\u00e8re des donn\u00e9es dans cette table et lit ces donn\u00e9es."
            ],
            "metadata": {
                "id": "rF9upA8avxDO"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- R\u00e9cup\u00e9rer les connexions \u00e0 la base de donn\u00e9es PostgreSQL qui ont un identifiant qui contient la cha\u00eene de caract\u00e8res \"MY_DATABASE_CONN\". Pour chaque connexion, cr\u00e9er un DAG dynamiquement en appelant la fonction create_dag(). Les DAGs cr\u00e9\u00e9s ont un identifiant \"pg_dynamic_\" suivi de l'identifiant de connexion PostgreSQL."
            ],
            "metadata": {
                "id": "InO7dsfH66Ki"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- R\u00e9cup\u00e9rer les tables de chaque connexion PostgreSQL cr\u00e9\u00e9e et cr\u00e9er un objet Dataset pour chacune d'entre elles. Regrouper tous les objets Dataset dans une liste all_pg_datasets. "
            ],
            "metadata": {
                "id": "_2I3LYNK83KT"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- Parcourir la liste des connexions conns, extraire le suffixe de chaque nom de connexion pour former le nom de la table correspondante, puis cr\u00e9e une instance de Dataset pour chaque connexion en utilisant la classe Dataset d'Airflow. Cette liste de datasets est stock\u00e9e dans la variable all_pg_datasets."
            ],
            "metadata": {
                "id": "UI9c6Skx9JNH"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- Cr\u00e9e une instance de DAG qui a pour schedule la liste all_pg_datasets cr\u00e9\u00e9e pr\u00e9c\u00e9demment. La liste all_pg_datasets contient des objets Dataset qui pointent vers des tables dans une base de donn\u00e9es Postgres. Cette m\u00e9thode permet de d\u00e9finir dynamiquement les tables \u00e0 lire en fonction des connexions trouv\u00e9es pr\u00e9c\u00e9demment.\n"
            ],
            "metadata": {
                "id": "Lp8bZvnu9lBZ"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- Cr\u00e9e une fonction qui effectue une lecture de donn\u00e9es depuis des tables PostgreSQL stock\u00e9es dans plusieurs bases de donn\u00e9es diff\u00e9rentes, qui sont identifi\u00e9es dans la liste conns.Elle bouclera sur chaque \u00e9l\u00e9ment de conns, extrait le suffixe de nom de base de donn\u00e9es \u00e0 partir de l'\u00e9l\u00e9ment, compose le nom de la table \u00e0 partir du suffixe, cr\u00e9e un hook de connexion Postgres \u00e0 la base de donn\u00e9es, ex\u00e9cute une requ\u00eate SQL pour extraire toutes les donn\u00e9es de la table, puis boucle sur chaque ligne extraite et affiche son contenu."
            ],
            "metadata": {
                "id": "KmDt7Y8--FCN"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "- Definir un PythonOperator, qui appelle la fonction read_from_postgres() d\u00e9finie pr\u00e9c\u00e9demment lorsque la t\u00e2che est ex\u00e9cut\u00e9e."
            ],
            "metadata": {
                "id": "27Y4hKx3-Nos"
            }
        }
    ]
}