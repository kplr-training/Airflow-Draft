{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/kplr-training/Airflow/blob/main/Ateliers/Solutions/04-Pipeline%20-%20Postgres.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Construction d'une pipeline d'ex\u00e9cution avec PostgreSQL "
            ],
            "metadata": {
                "id": "jFmbMwx4YVQF"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "![atelier4](https://user-images.githubusercontent.com/123757632/231912146-232774b4-c30c-4901-b4c1-ea27db59122b.png)"
            ],
            "metadata": {
                "id": "hE56uhOTYWiT"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## T\u00e2ches de cr\u00e9ation de table\n",
                "\n",
                "Il est possible d'utiliser PostgresOperator afin de d\u00e9finir des t\u00e2ches qui cr\u00e9eront des tables dans la base de donn\u00e9es postgres.\n",
                "\n",
                "Il est pr\u00e9vu de cr\u00e9er deux tables : une pour faciliter le nettoyage des donn\u00e9es (employees_temp) et une autre pour stocker les donn\u00e9es nettoy\u00e9es (employees)."
            ],
            "metadata": {
                "id": "UjgMOz6oZG_l"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AwIBDoKvYP59"
            },
            "outputs": [],
            "source": [
                "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
                "\n",
                "create_employees_table = PostgresOperator(\n",
                "    task_id=\"create_employees_table\",\n",
                "    postgres_conn_id=\"tutorial_pg_conn\",\n",
                "    sql=\"\"\"\n",
                "        CREATE TABLE IF NOT EXISTS employees (\n",
                "            \"Serial Number\" NUMERIC PRIMARY KEY,\n",
                "            \"Company Name\" TEXT,\n",
                "            \"Employee Markme\" TEXT,\n",
                "            \"Description\" TEXT,\n",
                "            \"Leave\" INTEGER\n",
                "        );\"\"\",\n",
                ")\n",
                "\n",
                "create_employees_temp_table = PostgresOperator(\n",
                "    task_id=\"create_employees_temp_table\",\n",
                "    postgres_conn_id=\"tutorial_pg_conn\",\n",
                "    sql=\"\"\"\n",
                "        DROP TABLE IF EXISTS employees_temp;\n",
                "        CREATE TABLE employees_temp (\n",
                "            \"Serial Number\" NUMERIC PRIMARY KEY,\n",
                "            \"Company Name\" TEXT,\n",
                "            \"Employee Markme\" TEXT,\n",
                "            \"Description\" TEXT,\n",
                "            \"Leave\" INTEGER\n",
                "        );\"\"\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## T\u00e2che de r\u00e9cup\u00e9ration de donn\u00e9es\n",
                "\n",
                "Ici, les donn\u00e9es sont r\u00e9cup\u00e9r\u00e9es, enregistr\u00e9es dans un fichier sur l'instance Airflow, puis charg\u00e9es \u00e0 partir de ce fichier dans une table interm\u00e9diaire afin de pouvoir ex\u00e9cuter les \u00e9tapes de nettoyage des donn\u00e9es."
            ],
            "metadata": {
                "id": "upY50tPvZLGc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import requests\n",
                "from airflow.decorators import task\n",
                "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
                "\n",
                "\n",
                "@task\n",
                "def get_data():\n",
                "    # REMARQUE : configurez cela selon les besoins de votre environnement Airflow.\n",
                "    data_path = \"/opt/airflow/dags/files/employees.csv\"\n",
                "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
                "\n",
                "    url = \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\"\n",
                "\n",
                "    response = requests.request(\"GET\", url)\n",
                "\n",
                "    with open(data_path, \"w\") as file:\n",
                "        file.write(response.text)\n",
                "\n",
                "    postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
                "    conn = postgres_hook.get_conn()\n",
                "    cur = conn.cursor()\n",
                "    with open(data_path, \"r\") as file:\n",
                "        cur.copy_expert(\n",
                "            \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n",
                "            file,\n",
                "        )\n",
                "    conn.commit()"
            ],
            "metadata": {
                "id": "82NvpgKuZNRS"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## T\u00e2che de fusion de donn\u00e9es\n",
                "Ici, des enregistrements compl\u00e8tement uniques sont s\u00e9lectionn\u00e9s \u00e0 partir des donn\u00e9es r\u00e9cup\u00e9r\u00e9es, puis il est v\u00e9rifi\u00e9 si des num\u00e9ros de s\u00e9rie d'employ\u00e9s sont d\u00e9j\u00e0 pr\u00e9sents dans la base de donn\u00e9es. Si tel est le cas, ces enregistrements sont mis \u00e0 jour avec les nouvelles donn\u00e9es."
            ],
            "metadata": {
                "id": "wb_j7GcyZRik"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from airflow.decorators import task\n",
                "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
                "\n",
                "\n",
                "@task\n",
                "def merge_data():\n",
                "    query = \"\"\"\n",
                "        INSERT INTO employees\n",
                "        SELECT *\n",
                "        FROM (\n",
                "            SELECT DISTINCT *\n",
                "            FROM employees_temp\n",
                "        ) t\n",
                "        ON CONFLICT (\"Serial Number\") DO UPDATE\n",
                "        SET\n",
                "              \"Employee Markme\" = excluded.\"Employee Markme\",\n",
                "              \"Description\" = excluded.\"Description\",\n",
                "              \"Leave\" = excluded.\"Leave\";\n",
                "    \"\"\"\n",
                "    try:\n",
                "        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
                "        conn = postgres_hook.get_conn()\n",
                "        cur = conn.cursor()\n",
                "        cur.execute(query)\n",
                "        conn.commit()\n",
                "        return 0\n",
                "    except Exception as e:\n",
                "        return 1"
            ],
            "metadata": {
                "id": "4yhYtQaqZSc2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Finalisation du DAG\n",
                "\n",
                "Les t\u00e2ches ont \u00e9t\u00e9 d\u00e9velopp\u00e9es et doivent maintenant \u00eatre int\u00e9gr\u00e9es dans un DAG. Ce DAG permet de d\u00e9finir quand et comment les t\u00e2ches doivent s'ex\u00e9cuter, ainsi que les d\u00e9pendances entre les t\u00e2ches. Le DAG ci-dessous est configur\u00e9 pour :\n",
                "\n",
                "* s'ex\u00e9cuter tous les jours \u00e0 minuit \u00e0 partir du 24er mars 2023,\n",
                "\n",
                "* ne s'ex\u00e9cuter qu'une seule fois en cas de jours manqu\u00e9s, et\n",
                "\n",
                "* s'interrompre apr\u00e8s 60 minutes.\n",
                "\n",
                "\u00c0 partir de la derni\u00e8re ligne de la d\u00e9finition du DAG \"process-employees\", il est possible de voir :"
            ],
            "metadata": {
                "id": "cgocEngSZU3_"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "[create_employees_table, create_employees_temp_table] >> get_data() >> merge_data()"
            ],
            "metadata": {
                "id": "qh_l_fqaZXRv"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "* la t\u00e2che \"merge_data()\" d\u00e9pend de la t\u00e2che \"get_data()\",\n",
                "\n",
                "* \"get_data()\" d\u00e9pend \u00e0 la fois des t\u00e2ches \"create_employees_table\" et \"create_employees_temp_table\",\n",
                "\n",
                "* et les t\u00e2ches \"create_employees_table\" et \"create_employees_temp_table\" peuvent s'ex\u00e9cuter ind\u00e9pendamment.\n",
                "\n",
                "En combinant tous ces \u00e9l\u00e9ments, le DAG est finalis\u00e9."
            ],
            "metadata": {
                "id": "bksL7gSzZcrt"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import datetime\n",
                "import pendulum\n",
                "import os\n",
                "\n",
                "import requests\n",
                "from airflow.decorators import dag, task\n",
                "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
                "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
                "\n",
                "\n",
                "@dag(\n",
                "    dag_id=\"process-employees\",\n",
                "    schedule_interval=\"0 0 * * *\",\n",
                "    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n",
                "    catchup=False,\n",
                "    dagrun_timeout=datetime.timedelta(minutes=60),\n",
                ")\n",
                "def ProcessEmployees():\n",
                "    create_employees_table = PostgresOperator(\n",
                "        task_id=\"create_employees_table\",\n",
                "        postgres_conn_id=\"tutorial_pg_conn\",\n",
                "        sql=\"\"\"\n",
                "            CREATE TABLE IF NOT EXISTS employees (\n",
                "                \"Serial Number\" NUMERIC PRIMARY KEY,\n",
                "                \"Company Name\" TEXT,\n",
                "                \"Employee Markme\" TEXT,\n",
                "                \"Description\" TEXT,\n",
                "                \"Leave\" INTEGER\n",
                "            );\"\"\",\n",
                "    )\n",
                "\n",
                "    create_employees_temp_table = PostgresOperator(\n",
                "        task_id=\"create_employees_temp_table\",\n",
                "        postgres_conn_id=\"tutorial_pg_conn\",\n",
                "        sql=\"\"\"\n",
                "            DROP TABLE IF EXISTS employees_temp;\n",
                "            CREATE TABLE employees_temp (\n",
                "                \"Serial Number\" NUMERIC PRIMARY KEY,\n",
                "                \"Company Name\" TEXT,\n",
                "                \"Employee Markme\" TEXT,\n",
                "                \"Description\" TEXT,\n",
                "                \"Leave\" INTEGER\n",
                "            );\"\"\",\n",
                "    )\n",
                "\n",
                "    @task\n",
                "    def get_data():\n",
                "        # REMARQUE : configurez cela selon les besoins de votre environnement Airflow.\n",
                "        data_path = \"/opt/airflow/dags/files/employees.csv\"\n",
                "        os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
                "\n",
                "        url = \"https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/tutorial/pipeline_example.csv\"\n",
                "\n",
                "        response = requests.request(\"GET\", url)\n",
                "\n",
                "        with open(data_path, \"w\") as file:\n",
                "            file.write(response.text)\n",
                "\n",
                "        postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
                "        conn = postgres_hook.get_conn()\n",
                "        cur = conn.cursor()\n",
                "        with open(data_path, \"r\") as file:\n",
                "            cur.copy_expert(\n",
                "                \"COPY employees_temp FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE '\\\"'\",\n",
                "                file,\n",
                "            )\n",
                "        conn.commit()\n",
                "\n",
                "    @task\n",
                "    def merge_data():\n",
                "        query = \"\"\"\n",
                "            INSERT INTO employees\n",
                "            SELECT *\n",
                "            FROM (\n",
                "                SELECT DISTINCT *\n",
                "                FROM employees_temp\n",
                "            ) t\n",
                "            ON CONFLICT (\"Serial Number\") DO UPDATE\n",
                "            SET \"Serial Number\" = excluded.\"Serial Number\";\n",
                "        \"\"\"\n",
                "        try:\n",
                "            postgres_hook = PostgresHook(postgres_conn_id=\"tutorial_pg_conn\")\n",
                "            conn = postgres_hook.get_conn()\n",
                "            cur = conn.cursor()\n",
                "            cur.execute(query)\n",
                "            conn.commit()\n",
                "            return 0\n",
                "        except Exception as e:\n",
                "            return 1\n",
                "\n",
                "    [create_employees_table, create_employees_temp_table] >> get_data() >> merge_data()\n",
                "\n",
                "\n",
                "dag = ProcessEmployees()"
            ],
            "metadata": {
                "id": "0Jkz2HXwZgXQ"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Enregistrez ce code dans un fichier python dans le dossier /dags (par exemple dags/process-employees.py) et (apr\u00e8s une courte attente), le DAG process-employees sera inclus dans la liste des DAG disponibles sur l'interface utilisateur web.\n",
                "\n"
            ],
            "metadata": {
                "id": "O1mHRHrwZjwW"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "![image](https://user-images.githubusercontent.com/123757632/227974881-9e246848-cd68-4215-9dc6-e3083042455e.png)\n"
            ],
            "metadata": {
                "id": "NDTsoZQKZoGI"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Il est possible de d\u00e9clencher le DAG \"process-employees\" en le s\u00e9lectionnant (via le curseur \u00e0 gauche) et en le lan\u00e7ant (en appuyant sur le bouton Ex\u00e9cuter sous Actions)."
            ],
            "metadata": {
                "id": "9yT5uGssZsGt"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "![image](https://user-images.githubusercontent.com/123757632/227974915-f7f4f7f6-6401-4ed1-a435-5c6db720d48c.png)\n"
            ],
            "metadata": {
                "id": "KSgH21jmZtc5"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "En examinant la vue en grille du DAG \"process-employees\", il est possible de constater que toutes les t\u00e2ches ont \u00e9t\u00e9 ex\u00e9cut\u00e9es avec succ\u00e8s lors de toutes les ex\u00e9cutions effectu\u00e9es. C'est un succ\u00e8s !"
            ],
            "metadata": {
                "id": "zDDutpQjZwUI"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "![image](https://user-images.githubusercontent.com/123757632/227974958-41e5b7bb-c5a0-4d22-acd3-8ae67b561586.png)"
            ],
            "metadata": {
                "id": "RqTpu4m3Zz6U"
            }
        }
    ]
}