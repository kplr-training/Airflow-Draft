{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic DAG Creation : Création dynamique de DAGs pour écrire et lire des données dans PostgreSQL en utilisant les hooks "
      ],
      "metadata": {
        "id": "ofDcoQhgsD1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pgaware_multi_dynamic_hook](https://user-images.githubusercontent.com/123757632/231905847-f4620b8c-b165-4d35-9b9e-afa22bc73a71.png)"
      ],
      "metadata": {
        "id": "deHhU2-isFHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pendulum import datetime\n",
        "from airflow import DAG, settings, Dataset\n",
        "from airflow.models import Connection\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
        "\n",
        "def create_dag(dag_id, schedule, pg_conn_id, default_args):\n",
        "    suffix = pg_conn_id[-2:]\n",
        "    table_name = \"table_stock_\"+suffix\n",
        "\n",
        "    pg_dataset=[Dataset(f\"postgres://airflow:airflow@postgres:5432/mydatabase{suffix}?table={table_name}\")]\n"
      ],
      "metadata": {
        "id": "wA4oStdk_i4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction create_dag commence par extraire le suffixe de l'ID de connexion Postgres (pg_conn_id) pour obtenir le nom de la table à utiliser. Elle crée ensuite un objet Dataset à partir de cette table en utilisant la classe Dataset fournie par Airflow. Cette classe permet de manipuler des données stockées dans différents types de sources de données (bases de données, fichiers, etc.) et de les passer entre les opérateurs du DAG.\n",
        "\n",
        "Dans ce cas, l'objet Dataset est créé en utilisant la chaîne de connexion à la base de données Postgres, qui comprend le nom d'utilisateur (airflow), le mot de passe (airflow), le nom de l'hôte (postgres) et le port (5432). Le nom de la base de données est construit à partir du suffixe extrait de l'ID de connexion Postgres. La chaîne \"?table={table_name}\" est utilisée pour spécifier la table qui sera lue ou écrite."
      ],
      "metadata": {
        "id": "DycTT96V_mmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def write_to_postgres(*args):\n",
        "        print(\"Hello Dynamic Postgre DAGS\")\n",
        "        print(\"This is DAG: {}\".format(str(pg_conn_id)))\n",
        "\n",
        "        # Create a PostgresHook\n",
        "        hook = PostgresHook(postgres_conn_id=pg_conn_id)\n",
        "\n",
        "        # Execute a query\n",
        "        query = (f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (price TEXT,date TEXT)\"\"\")\n",
        "        hook.run(query)\n",
        "\n",
        "        query = (f\"INSERT INTO {table_name} (col1, col2) VALUES ('price_stock_{suffix}', 'date_stock_{suffix}')\")\n",
        "        hook.run(query)\n",
        "\n",
        "        query = (f\"SELECT * FROM {table_name}\")\n",
        "        rows = hook.get_records(query)\n",
        "        for row in rows:\n",
        "                print(row)"
      ],
      "metadata": {
        "id": "ljB8tDm6_7oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction **write_to_postgres** se connecte à une base de données Postgres via PostgresHook, exécute des requêtes SQL et affiche les résultats.\n",
        "\n",
        "Elle prend un nombre variable d'arguments en entrée via l'argument *args, mais ne les utilise pas. Elle affiche simplement un message de bienvenue et le nom de la connexion pg_conn_id fourni en paramètre.\n",
        "\n",
        "**write_to_postgres** crée un objet PostgresHook pour la connexion pg_conn_id fournie et exécute trois requêtes SQL. La première requête crée une table nommée table_stock_<suffix> si elle n'existe pas déjà. La seconde requête insère une ligne dans la table avec les valeurs price_stock_<suffix> et date_stock_<suffix>. La troisième requête sélectionne toutes les lignes de la table et les affiche à l'écran avec la boucle for.\n",
        "\n",
        "La variable suffix est définie en extrayant les deux derniers caractères de la chaîne pg_conn_id. La variable table_name est définie comme table_stock_<suffix> pour éviter les conflits entre les tables créées par différentes instances de la fonction write_to_postgres."
      ],
      "metadata": {
        "id": "qnMXwb-r__ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dag = DAG(\n",
        "        dag_id,\n",
        "        schedule=schedule,\n",
        "        default_args=default_args)\n",
        "\n",
        "    with dag:\n",
        "      t1 = PythonOperator(\n",
        "            task_id=\"write_to_postgres\",\n",
        "            outlets=pg_dataset,\n",
        "            python_callable=write_to_postgres)\n",
        "\n",
        "    return dag"
      ],
      "metadata": {
        "id": "1JexnPLEAAEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création d'une instance de DAG avec une seule tâche définie, qui utilise la classe PythonOperator. Les arguments fournis à PythonOperator sont task_id, outlets et python_callable. task_id est l'identifiant unique pour la tâche, outlets est un ensemble de connexions à des systèmes externes que la tâche utilise, et python_callable est la fonction **write_to_postgres** Python à exécuter."
      ],
      "metadata": {
        "id": "onlYVO7ZACYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = settings.Session()\n",
        "conns = (\n",
        "    session.query(Connection.conn_id)\n",
        "    .filter(Connection.conn_id.ilike(\"%MY_DATABASE_CONN%\"))\n",
        "    .all()\n",
        ")"
      ],
      "metadata": {
        "id": "uoYM0nI1ACv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Récupération de toutes les connexions dont l'identifiant contient la chaîne de caractères \"MY_DATABASE_CONN\". Il crée une session pour interagir avec la base de données d'Airflow et exécute une requête pour récupérer les identifiants de connexions correspondantes. Les résultats sont stockés dans la variable **conns**."
      ],
      "metadata": {
        "id": "AOH4a80lAsUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for conn in conns:\n",
        "\n",
        "    # BEWARE : the returned connection ID format is messed UP \n",
        "    # and needs to be cleansed and sanitized first \n",
        "    # otherwise all the rest of the code will break.\n",
        "    conn = str(conn).strip(\"(),'\")\n",
        "\n",
        "    dag_id = \"pg_dynamic_{}\".format(conn)\n",
        "\n",
        "    default_args = {\"owner\": \"airflow\", \"start_date\": datetime(2023, 1, 1)}\n",
        "\n",
        "    schedule = \"@daily\"\n",
        "    pg_conn_id = conn\n",
        "\n",
        "    globals()[dag_id] = create_dag(dag_id, schedule, pg_conn_id, default_args)\n",
        "\n",
        "\n",
        "# Prepare pg_datasets\n",
        "all_pg_datasets=[]"
      ],
      "metadata": {
        "id": "mik3nJV8Asr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour chaque identifiant de connexion dans la liste conns, on nettoie la chaîne de caractères de l'identifiant et crée un nouvel identifiant de DAG unique. Ensuite, on définit des paramètres par défaut pour les DAG (propriétaire, date de début, etc.) et crée le DAG en appelant la fonction **create_dag** avec les paramètres appropriés.\n",
        "\n",
        "Enfin, le code crée une liste vide **all_pg_datasets** pour stocker tous les jeux de données Postgres associés à chaque DAG créé ultérieurement."
      ],
      "metadata": {
        "id": "o4R03tcVAvLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for conn in conns:\n",
        "    conn = str(conn).strip(\"(),'\")\n",
        "    suffix = conn[-2:]\n",
        "    table_name = \"table_stock_\"+suffix\n",
        "    all_pg_datasets.append(Dataset(f\"postgres://airflow:airflow@postgres:5432/mydatabase{suffix}?table={table_name}\"))"
      ],
      "metadata": {
        "id": "kfx5Mbo5AvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour chaque identifiant de connexion dans la liste conns, on nettoie la chaîne de caractères de l'identifiant et extrait les deux derniers caractères pour créer un suffixe unique. Ensuite, on crée un nom de table Postgres unique en utilisant ce suffixe et stocke un nouveau dataset Postgres dans la liste **all_pg_datasets** en utilisant la classe Dataset. Le dataset contient l'URL de connexion à la base de données Postgres correspondante, avec le nom de la table Postgres correspondant à **table_name**."
      ],
      "metadata": {
        "id": "bKWpaYGYAwQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with DAG(\n",
        "    'read_from_postgres_hook_aware_MULTI',\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    schedule = all_pg_datasets\n",
        ") as dag:\n",
        "   \n",
        "    def read_from_postgres():\n",
        "\n",
        "        for conn in conns:\n",
        "            conn = str(conn).strip(\"(),'\")\n",
        "            suffix = conn[-2:]\n",
        "            table_name = \"table_stock_\"+suffix\n",
        "\n",
        "            # Execute hook\n",
        "            hook = PostgresHook(postgres_conn_id=conn)\n",
        "\n",
        "            # Execute query\n",
        "            query = (f\"SELECT * FROM {table_name}\")\n",
        "            rows = hook.get_records(query)\n",
        "            for row in rows:\n",
        "                    print(row)\n"
      ],
      "metadata": {
        "id": "9uAL5uEhAwkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créer un DAG Airflow qui effectue la lecture de données à partir de tables spécifiques dans une base de données Postgres. Le DAG est planifié pour s'exécuter en fonction de la fréquence de chaque jeu de données Postgres, définie dans la liste all_pg_datasets.\n",
        "\n",
        "Pour chaque connexion, le code crée un suffixe unique et un nom de table correspondant. Ensuite, il crée un hook Postgres en utilisant l'identifiant de connexion Postgres correspondant et exécute une requête SQL en utilisant le nom de table unique. Les données résultantes sont récupérées en utilisant la méthode get_records de l'objet hook Postgres, puis chaque ligne est imprimée sur la console en utilisant la fonction read_from_postgres.\n",
        "\n",
        "Le DAG est nommé \"read_from_postgres_hook_aware_MULTI\" et une date de début datetime est définie."
      ],
      "metadata": {
        "id": "j0NXKbOLAzju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "read_task = PythonOperator(\n",
        "        task_id='read_task',\n",
        "        python_callable=read_from_postgres,\n",
        "        dag=dag\n",
        "    )"
      ],
      "metadata": {
        "id": "gh1nFHRYAz11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création de la tâche \"read_task\" pour exécuter la fonction **read_from_postgres** définie précédemment en tant que callable Python."
      ],
      "metadata": {
        "id": "DHzekJTVA19f"
      }
    }
  ]
}