{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/kplr-training/Airflow/blob/main/Ateliers/Solutions/11-Data%20Aware%20Scheduling%20-%20Database.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Dynamic DAG Creation\u202f: Cr\u00e9ation dynamique de DAGs pour \u00e9crire et lire des donn\u00e9es dans PostgreSQL en utilisant les hooks "
            ],
            "metadata": {
                "id": "ofDcoQhgsD1t"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "![pgaware_multi_dynamic_hook](https://user-images.githubusercontent.com/123757632/231905847-f4620b8c-b165-4d35-9b9e-afa22bc73a71.png)"
            ],
            "metadata": {
                "id": "deHhU2-isFHl"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pendulum import datetime\n",
                "from airflow import DAG, settings, Dataset\n",
                "from airflow.models import Connection\n",
                "from airflow.operators.python import PythonOperator\n",
                "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
                "\n",
                "def create_dag(dag_id, schedule, pg_conn_id, default_args):\n",
                "    suffix = pg_conn_id[-2:]\n",
                "    table_name = \"table_stock_\"+suffix\n",
                "\n",
                "    pg_dataset=[Dataset(f\"postgres://airflow:airflow@postgres:5432/mydatabase{suffix}?table={table_name}\")]\n"
            ],
            "metadata": {
                "id": "wA4oStdk_i4E"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "La fonction create_dag commence par extraire le suffixe de l'ID de connexion Postgres (pg_conn_id) pour obtenir le nom de la table \u00e0 utiliser. Elle cr\u00e9e ensuite un objet Dataset \u00e0 partir de cette table en utilisant la classe Dataset fournie par Airflow. Cette classe permet de manipuler des donn\u00e9es stock\u00e9es dans diff\u00e9rents types de sources de donn\u00e9es (bases de donn\u00e9es, fichiers, etc.) et de les passer entre les op\u00e9rateurs du DAG.\n",
                "\n",
                "Dans ce cas, l'objet Dataset est cr\u00e9\u00e9 en utilisant la cha\u00eene de connexion \u00e0 la base de donn\u00e9es Postgres, qui comprend le nom d'utilisateur (airflow), le mot de passe (airflow), le nom de l'h\u00f4te (postgres) et le port (5432). Le nom de la base de donn\u00e9es est construit \u00e0 partir du suffixe extrait de l'ID de connexion Postgres. La cha\u00eene \"?table={table_name}\" est utilis\u00e9e pour sp\u00e9cifier la table qui sera lue ou \u00e9crite."
            ],
            "metadata": {
                "id": "DycTT96V_mmg"
            }
        },
        {
            "cell_type": "code",
            "source": [
                " def write_to_postgres(*args):\n",
                "        print(\"Hello Dynamic Postgre DAGS\")\n",
                "        print(\"This is DAG: {}\".format(str(pg_conn_id)))\n",
                "\n",
                "        # Create a PostgresHook\n",
                "        hook = PostgresHook(postgres_conn_id=pg_conn_id)\n",
                "\n",
                "        # Execute a query\n",
                "        query = (f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (price TEXT,date TEXT)\"\"\")\n",
                "        hook.run(query)\n",
                "\n",
                "        query = (f\"INSERT INTO {table_name} (col1, col2) VALUES ('price_stock_{suffix}', 'date_stock_{suffix}')\")\n",
                "        hook.run(query)\n",
                "\n",
                "        query = (f\"SELECT * FROM {table_name}\")\n",
                "        rows = hook.get_records(query)\n",
                "        for row in rows:\n",
                "                print(row)"
            ],
            "metadata": {
                "id": "ljB8tDm6_7oO"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "La fonction **write_to_postgres** se connecte \u00e0 une base de donn\u00e9es Postgres via PostgresHook, ex\u00e9cute des requ\u00eates SQL et affiche les r\u00e9sultats.\n",
                "\n",
                "Elle prend un nombre variable d'arguments en entr\u00e9e via l'argument *args, mais ne les utilise pas. Elle affiche simplement un message de bienvenue et le nom de la connexion pg_conn_id fourni en param\u00e8tre.\n",
                "\n",
                "**write_to_postgres** cr\u00e9e un objet PostgresHook pour la connexion pg_conn_id fournie et ex\u00e9cute trois requ\u00eates SQL. La premi\u00e8re requ\u00eate cr\u00e9e une table nomm\u00e9e table_stock_<suffix> si elle n'existe pas d\u00e9j\u00e0. La seconde requ\u00eate ins\u00e8re une ligne dans la table avec les valeurs price_stock_<suffix> et date_stock_<suffix>. La troisi\u00e8me requ\u00eate s\u00e9lectionne toutes les lignes de la table et les affiche \u00e0 l'\u00e9cran avec la boucle for.\n",
                "\n",
                "La variable suffix est d\u00e9finie en extrayant les deux derniers caract\u00e8res de la cha\u00eene pg_conn_id. La variable table_name est d\u00e9finie comme table_stock_<suffix> pour \u00e9viter les conflits entre les tables cr\u00e9\u00e9es par diff\u00e9rentes instances de la fonction write_to_postgres."
            ],
            "metadata": {
                "id": "qnMXwb-r__ZU"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "dag = DAG(\n",
                "        dag_id,\n",
                "        schedule=schedule,\n",
                "        default_args=default_args)\n",
                "\n",
                "    with dag:\n",
                "      t1 = PythonOperator(\n",
                "            task_id=\"write_to_postgres\",\n",
                "            outlets=pg_dataset,\n",
                "            python_callable=write_to_postgres)\n",
                "\n",
                "    return dag"
            ],
            "metadata": {
                "id": "1JexnPLEAAEr"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Cr\u00e9ation d'une instance de DAG avec une seule t\u00e2che d\u00e9finie, qui utilise la classe PythonOperator. Les arguments fournis \u00e0 PythonOperator sont task_id, outlets et python_callable. task_id est l'identifiant unique pour la t\u00e2che, outlets est un ensemble de connexions \u00e0 des syst\u00e8mes externes que la t\u00e2che utilise, et python_callable est la fonction **write_to_postgres** Python \u00e0 ex\u00e9cuter."
            ],
            "metadata": {
                "id": "onlYVO7ZACYl"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "session = settings.Session()\n",
                "conns = (\n",
                "    session.query(Connection.conn_id)\n",
                "    .filter(Connection.conn_id.ilike(\"%MY_DATABASE_CONN%\"))\n",
                "    .all()\n",
                ")"
            ],
            "metadata": {
                "id": "uoYM0nI1ACv8"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "R\u00e9cup\u00e9ration de toutes les connexions dont l'identifiant contient la cha\u00eene de caract\u00e8res \"MY_DATABASE_CONN\". Il cr\u00e9e une session pour interagir avec la base de donn\u00e9es d'Airflow et ex\u00e9cute une requ\u00eate pour r\u00e9cup\u00e9rer les identifiants de connexions correspondantes. Les r\u00e9sultats sont stock\u00e9s dans la variable **conns**."
            ],
            "metadata": {
                "id": "AOH4a80lAsUR"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "for conn in conns:\n",
                "\n",
                "    # BEWARE : the returned connection ID format is messed UP \n",
                "    # and needs to be cleansed and sanitized first \n",
                "    # otherwise all the rest of the code will break.\n",
                "    conn = str(conn).strip(\"(),'\")\n",
                "\n",
                "    dag_id = \"pg_dynamic_{}\".format(conn)\n",
                "\n",
                "    default_args = {\"owner\": \"airflow\", \"start_date\": datetime(2023, 1, 1)}\n",
                "\n",
                "    schedule = \"@daily\"\n",
                "    pg_conn_id = conn\n",
                "\n",
                "    globals()[dag_id] = create_dag(dag_id, schedule, pg_conn_id, default_args)\n",
                "\n",
                "\n",
                "# Prepare pg_datasets\n",
                "all_pg_datasets=[]"
            ],
            "metadata": {
                "id": "mik3nJV8Asr2"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Pour chaque identifiant de connexion dans la liste conns, on nettoie la cha\u00eene de caract\u00e8res de l'identifiant et cr\u00e9e un nouvel identifiant de DAG unique. Ensuite, on d\u00e9finit des param\u00e8tres par d\u00e9faut pour les DAG (propri\u00e9taire, date de d\u00e9but, etc.) et cr\u00e9e le DAG en appelant la fonction **create_dag** avec les param\u00e8tres appropri\u00e9s.\n",
                "\n",
                "Enfin, le code cr\u00e9e une liste vide **all_pg_datasets** pour stocker tous les jeux de donn\u00e9es Postgres associ\u00e9s \u00e0 chaque DAG cr\u00e9\u00e9 ult\u00e9rieurement."
            ],
            "metadata": {
                "id": "o4R03tcVAvLr"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "for conn in conns:\n",
                "    conn = str(conn).strip(\"(),'\")\n",
                "    suffix = conn[-2:]\n",
                "    table_name = \"table_stock_\"+suffix\n",
                "    all_pg_datasets.append(Dataset(f\"postgres://airflow:airflow@postgres:5432/mydatabase{suffix}?table={table_name}\"))"
            ],
            "metadata": {
                "id": "kfx5Mbo5AvjP"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Pour chaque identifiant de connexion dans la liste conns, on nettoie la cha\u00eene de caract\u00e8res de l'identifiant et extrait les deux derniers caract\u00e8res pour cr\u00e9er un suffixe unique. Ensuite, on cr\u00e9e un nom de table Postgres unique en utilisant ce suffixe et stocke un nouveau dataset Postgres dans la liste **all_pg_datasets** en utilisant la classe Dataset. Le dataset contient l'URL de connexion \u00e0 la base de donn\u00e9es Postgres correspondante, avec le nom de la table Postgres correspondant \u00e0 **table_name**."
            ],
            "metadata": {
                "id": "bKWpaYGYAwQb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "with DAG(\n",
                "    'read_from_postgres_hook_aware_MULTI',\n",
                "    start_date=datetime(2023, 1, 1),\n",
                "    schedule = all_pg_datasets\n",
                ") as dag:\n",
                "   \n",
                "    def read_from_postgres():\n",
                "\n",
                "        for conn in conns:\n",
                "            conn = str(conn).strip(\"(),'\")\n",
                "            suffix = conn[-2:]\n",
                "            table_name = \"table_stock_\"+suffix\n",
                "\n",
                "            # Execute hook\n",
                "            hook = PostgresHook(postgres_conn_id=conn)\n",
                "\n",
                "            # Execute query\n",
                "            query = (f\"SELECT * FROM {table_name}\")\n",
                "            rows = hook.get_records(query)\n",
                "            for row in rows:\n",
                "                    print(row)\n"
            ],
            "metadata": {
                "id": "9uAL5uEhAwkn"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Cr\u00e9er un DAG Airflow qui effectue la lecture de donn\u00e9es \u00e0 partir de tables sp\u00e9cifiques dans une base de donn\u00e9es Postgres. Le DAG est planifi\u00e9 pour s'ex\u00e9cuter en fonction de la fr\u00e9quence de chaque jeu de donn\u00e9es Postgres, d\u00e9finie dans la liste all_pg_datasets.\n",
                "\n",
                "Pour chaque connexion, le code cr\u00e9e un suffixe unique et un nom de table correspondant. Ensuite, il cr\u00e9e un hook Postgres en utilisant l'identifiant de connexion Postgres correspondant et ex\u00e9cute une requ\u00eate SQL en utilisant le nom de table unique. Les donn\u00e9es r\u00e9sultantes sont r\u00e9cup\u00e9r\u00e9es en utilisant la m\u00e9thode get_records de l'objet hook Postgres, puis chaque ligne est imprim\u00e9e sur la console en utilisant la fonction read_from_postgres.\n",
                "\n",
                "Le DAG est nomm\u00e9 \"read_from_postgres_hook_aware_MULTI\" et une date de d\u00e9but datetime est d\u00e9finie."
            ],
            "metadata": {
                "id": "j0NXKbOLAzju"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "read_task = PythonOperator(\n",
                "        task_id='read_task',\n",
                "        python_callable=read_from_postgres,\n",
                "        dag=dag\n",
                "    )"
            ],
            "metadata": {
                "id": "gh1nFHRYAz11"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Cr\u00e9ation de la t\u00e2che \"read_task\" pour ex\u00e9cuter la fonction **read_from_postgres** d\u00e9finie pr\u00e9c\u00e9demment en tant que callable Python."
            ],
            "metadata": {
                "id": "DHzekJTVA19f"
            }
        }
    ]
}